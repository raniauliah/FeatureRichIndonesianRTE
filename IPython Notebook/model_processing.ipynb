{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtj-qs-m6Mk8"
      },
      "source": [
        "# Import Dependencies and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9brh4JW7_zo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import scipy.sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from IPython.display import clear_output\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tag import CRFTagger\n",
        "from sklearn_crfsuite import CRF"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZvrbP3_gbC"
      },
      "source": [
        "## Load Word Embedding (Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG7ZaNS7ACcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150f52cc-7512-468c-962e-eb9220e2bc3c"
      },
      "source": [
        "resource_path = \"/content/resources/\"\n",
        "model_path = \"idwiki_word2vec_300.model\"\n",
        "\n",
        "modelword2vec = Word2Vec.load(resource_path + model_path)\n",
        "w2v = dict(zip(modelword2vec.wv.index2word, modelword2vec.wv.syn0))\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.dim = len(word2vec['dan'])\n",
        "        \n",
        "    def tokenize(self, sentences):\n",
        "        return [str(sentence).lower().split(\" \") for sentence in sentences]\n",
        "\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Ambil kata-katanya lalu rata-rata\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "vectorizer = MeanEmbeddingVectorizer(w2v)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCX8ciHS_eME"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVXaWbgi-zaK"
      },
      "source": [
        "data_path = \"/content/data_source/preprocessed_result/\"\n",
        "train_path = \"preprocessed_train.csv\"\n",
        "test_path = \"preprocessed_test.csv\"\n",
        "\n",
        "train = pd.read_csv(data_path + train_path)\n",
        "test = pd.read_csv(data_path + test_path)\n",
        "\n",
        "train = train.drop(['Unnamed: 0'], axis=1)\n",
        "test = test.drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dWsgJD6CiaQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b8ca30ef-6bec-4266-fada-f1fca2f4318a"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_A</th>\n",
              "      <th>sent_B</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed_A</th>\n",
              "      <th>preprocessed_B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pada awalnya bangsa Israel hanya terdiri dari ...</td>\n",
              "      <td>Pada awalnya bangsa Yahudi hanya terdiri dari ...</td>\n",
              "      <td>menolak perubahan teks terakhir oleh istimewa ...</td>\n",
              "      <td>0</td>\n",
              "      <td>awal bangsa israel diri satu kelompok keluarga...</td>\n",
              "      <td>awal bangsa yahudi diri satu kelompok keluarga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Salah satu tekniknya adalah periplus , deskrip...</td>\n",
              "      <td>Bangsa Romawi memberi sumbangan pada pemetaan ...</td>\n",
              "      <td>sejarah geografi</td>\n",
              "      <td>0</td>\n",
              "      <td>salah satu teknik periplus deskripsi labuh dar...</td>\n",
              "      <td>bangsa romawi beri sumbang meta mereka banyak ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>definisi anthropologi menurut para ahli</td>\n",
              "      <td>0</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar lukis cir...</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar sukusuku ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sekarang , tidak ada yang tahu pasti kapan sej...</td>\n",
              "      <td>Sejarah dimulai pada awal penciptaan manusia .</td>\n",
              "      <td>1 penggantian istilah quot prasejarah quot den...</td>\n",
              "      <td>0</td>\n",
              "      <td>sekarang ada tahu kapan sejarah mulai</td>\n",
              "      <td>sejarah mulai awal cipta manusia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Soekarno tidak memilih bahasanya sendiri , Jaw...</td>\n",
              "      <td>Dengan memilih Bahasa Melayu , para pejuang ke...</td>\n",
              "      <td>bahasa indonesia</td>\n",
              "      <td>0</td>\n",
              "      <td>soekarno pilih bahasa sendiri jawa benar bahas...</td>\n",
              "      <td>pilih bahasa melayu juang merdeka satu seperti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sent_A  ...                                     preprocessed_B\n",
              "0  Pada awalnya bangsa Israel hanya terdiri dari ...  ...  awal bangsa yahudi diri satu kelompok keluarga...\n",
              "1  Salah satu tekniknya adalah periplus , deskrip...  ...  bangsa romawi beri sumbang meta mereka banyak ...\n",
              "2  Etnolinguistik antropologi adalah ilmu yang me...  ...  etnolinguistik antropologi ilmu ajar sukusuku ...\n",
              "3  Sekarang , tidak ada yang tahu pasti kapan sej...  ...                   sejarah mulai awal cipta manusia\n",
              "4  Soekarno tidak memilih bahasanya sendiri , Jaw...  ...  pilih bahasa melayu juang merdeka satu seperti...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "toWlDuzSDPgq",
        "outputId": "d981a18a-5e73-4551-d75c-57fe934e2059"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_A</th>\n",
              "      <th>sent_B</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed_A</th>\n",
              "      <th>preprocessed_B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pada 1964 , setelah memegang berbagai jabatan ...</td>\n",
              "      <td>Pada 1964 , setelah memegang berbagai jabatan ...</td>\n",
              "      <td>latar belakang</td>\n",
              "      <td>0</td>\n",
              "      <td>1964 pegang bagai jabat perintah mesir pilih p...</td>\n",
              "      <td>1964 pegang bagai jabat perintah mesir pilih p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bahasa Indonesia adalah bahasa resmi Negara Re...</td>\n",
              "      <td>Bahasa Indonesia adalah bahasa Resmi Indonesia...</td>\n",
              "      <td>merapikan</td>\n",
              "      <td>1</td>\n",
              "      <td>bahasa indonesia bahasa resmi negara republik ...</td>\n",
              "      <td>bahasa indonesia bahasa resmi indonesia bagi k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Biologi menumpukan kepada ciri-ciri fisikal da...</td>\n",
              "      <td>Ilmu biologi berurusan dengan ciri-ciri fisik ...</td>\n",
              "      <td>menerjemahkan artikel dari bahasa malaysia ke ...</td>\n",
              "      <td>1</td>\n",
              "      <td>biologi tumpu ciriciri fisikal tabiat hidup ma...</td>\n",
              "      <td>ilmu biologi urus ciriciri fisik perilaku makh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Seluruh anggota batalion Bali tersebut tewas s...</td>\n",
              "      <td>Seluruh anggota batalion Bali tersebut hidup s...</td>\n",
              "      <td>sejarah</td>\n",
              "      <td>0</td>\n",
              "      <td>seluruh anggota batalion bal sebut tewas semua...</td>\n",
              "      <td>seluruh anggota batalion bal sebut hidup semua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Transportasi udara dilayani oleh Bandara Inter...</td>\n",
              "      <td>Transportasi udara dilayani oleh Bandara Inter...</td>\n",
              "      <td>transportasi</td>\n",
              "      <td>1</td>\n",
              "      <td>transportasi udara layan bandara internasional...</td>\n",
              "      <td>transportasi udara layan bandara internasional...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sent_A  ...                                     preprocessed_B\n",
              "0  Pada 1964 , setelah memegang berbagai jabatan ...  ...  1964 pegang bagai jabat perintah mesir pilih p...\n",
              "1  Bahasa Indonesia adalah bahasa resmi Negara Re...  ...  bahasa indonesia bahasa resmi indonesia bagi k...\n",
              "2  Biologi menumpukan kepada ciri-ciri fisikal da...  ...  ilmu biologi urus ciriciri fisik perilaku makh...\n",
              "3  Seluruh anggota batalion Bali tersebut tewas s...  ...  seluruh anggota batalion bal sebut hidup semua...\n",
              "4  Transportasi udara dilayani oleh Bandara Inter...  ...  transportasi udara layan bandara internasional...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2S_FIMoAk55"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qaHaBeDBg4i"
      },
      "source": [
        "## Distance Based: WMD\n",
        "\n",
        "Calculate the distance of the word embedidng using Word Mover's Distance (WMD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5GdfHI6Bi-P"
      },
      "source": [
        "def wmd(a,b):\n",
        "  return modelword2vec.wmdistance(a.split(),b.split())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVVDL33mCEfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de0479e-ec9a-4ead-cb79-9d9f19e19d4c"
      },
      "source": [
        "train['wmd'] = train.apply(lambda x: wmd(x['preprocessed_A'], x['preprocessed_B']), axis=1)\n",
        "test['wmd'] = test.apply(lambda x: wmd(x['preprocessed_A'], x['preprocessed_B']), axis=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLEfMVOgDmtL"
      },
      "source": [
        "## Token Based: raw_similar_tok, similar_tok, raw_diff_tok, diff_tok, num_tok_T, num_tok_H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTTVfR5PDoCI"
      },
      "source": [
        "*   Percentage of token similarity: raw (raw_similar_tok) & preprocessed (similar_tok)\n",
        "*   The difference in the number of tokens: raw (raw_diff_tok) & preprocessed (diff_tok)\n",
        "*   num of token in preprocessed text T (num_sentA) --> in paper: num_tok_T\n",
        "*   num of token in preprocessed hypothesis H (num_sentB) --> in paper: num_tok_H"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSHYPnQZJCqA"
      },
      "source": [
        "def check_token_func(df, similar_type):\n",
        "  if similar_type == 'preprocessed':\n",
        "    idx_1 = 'preprocessed_A'\n",
        "    idx_2 = 'preprocessed_B'\n",
        "  else:\n",
        "    idx_1 = 'sent_A'\n",
        "    idx_2 = 'sent_B'\n",
        "\n",
        "  list_premis = df[idx_1].values\n",
        "  list_hipotesis = df[idx_2].values\n",
        "  list_similar = []\n",
        "  list_diff = []\n",
        "  num_premis = []\n",
        "  num_hipotesis = []\n",
        "\n",
        "  for p, h in zip(list_premis, list_hipotesis):\n",
        "    # check percentage of similar words\n",
        "    p_split = p.split()\n",
        "    h_split = h.split()\n",
        "\n",
        "    similar = 0\n",
        "    for word in h_split:\n",
        "      if word in p_split:\n",
        "        similar += 1\n",
        "    similar_percentage = similar / (len(h_split))\n",
        "\n",
        "    # check difference of number of tokens in premis and hipotesis\n",
        "    diff = len(p_split) - len(h_split)\n",
        "    \n",
        "    list_diff.append(diff)\n",
        "    list_similar.append(similar_percentage)\n",
        "    num_premis.append(len(p_split))\n",
        "    num_hipotesis.append(len(h_split))\n",
        "    \n",
        "  return list_similar, list_diff, num_premis, num_hipotesis"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_bdrJQPDGS"
      },
      "source": [
        "similar_train, diff_train, train_num_A, train_num_B = check_token_func(train, 'preprocessed')\n",
        "raw_similar_train, raw_diff_train, raw_train_num_A, raw_train_num_B = check_token_func(train, 'raw')\n",
        "\n",
        "similar_test, diff_test, test_num_A, test_num_B = check_token_func(test, 'preprocessed')\n",
        "raw_similar_test, raw_diff_test, raw_test_num_A, raw_test_num_B = check_token_func(test, 'raw')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCyubc2SREFs"
      },
      "source": [
        "train['similar_tok'] = similar_train\n",
        "train['raw_similar_tok'] = raw_similar_train\n",
        "train['diff_tok'] = diff_train\n",
        "train['raw_diff_tok'] = raw_diff_train\n",
        "train['num_sentA'] = train_num_A\n",
        "train['num_sentB'] = train_num_B\n",
        "\n",
        "test['similar_tok'] = similar_test\n",
        "test['raw_similar_tok'] = raw_similar_test\n",
        "test['diff_tok'] = diff_test\n",
        "test['raw_diff_tok'] = raw_diff_test\n",
        "test['num_sentA'] = test_num_A\n",
        "test['num_sentB'] = test_num_B"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VN7JKJ2awZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "9fc86e70-eeb3-44fc-a50d-df155549acd6"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_A</th>\n",
              "      <th>sent_B</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed_A</th>\n",
              "      <th>preprocessed_B</th>\n",
              "      <th>wmd</th>\n",
              "      <th>similar_tok</th>\n",
              "      <th>raw_similar_tok</th>\n",
              "      <th>diff_tok</th>\n",
              "      <th>raw_diff_tok</th>\n",
              "      <th>num_sentA</th>\n",
              "      <th>num_sentB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pada awalnya bangsa Israel hanya terdiri dari ...</td>\n",
              "      <td>Pada awalnya bangsa Yahudi hanya terdiri dari ...</td>\n",
              "      <td>menolak perubahan teks terakhir oleh istimewa ...</td>\n",
              "      <td>0</td>\n",
              "      <td>awal bangsa israel diri satu kelompok keluarga...</td>\n",
              "      <td>awal bangsa yahudi diri satu kelompok keluarga...</td>\n",
              "      <td>0.989940</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Salah satu tekniknya adalah periplus , deskrip...</td>\n",
              "      <td>Bangsa Romawi memberi sumbangan pada pemetaan ...</td>\n",
              "      <td>sejarah geografi</td>\n",
              "      <td>0</td>\n",
              "      <td>salah satu teknik periplus deskripsi labuh dar...</td>\n",
              "      <td>bangsa romawi beri sumbang meta mereka banyak ...</td>\n",
              "      <td>21.542550</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>24</td>\n",
              "      <td>38</td>\n",
              "      <td>36</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>definisi anthropologi menurut para ahli</td>\n",
              "      <td>0</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar lukis cir...</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar sukusuku ...</td>\n",
              "      <td>9.921097</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sent_A  ... num_sentB\n",
              "0  Pada awalnya bangsa Israel hanya terdiri dari ...  ...        18\n",
              "1  Salah satu tekniknya adalah periplus , deskrip...  ...        12\n",
              "2  Etnolinguistik antropologi adalah ilmu yang me...  ...         9\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWV_DepuaTKm"
      },
      "source": [
        "## Token Based: same_unigram, same_bigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BskN3Z-waV-z"
      },
      "source": [
        "*   num of overlap unigrams on raw data of sent_A & sent_B (same_unigram)\n",
        "*   num of overlap bigrams on raw data of sent_A & sent_B (same_bigram)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwdR_363aW6l"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "def find_ngrams(input_list, n):\n",
        "    return list(zip(*[input_list[i:] for i in range(n)]))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5NfZxPbatq2"
      },
      "source": [
        "# generate unigram and bigram\n",
        "train['sentA_unigram'] = train['sent_A'].map(lambda x: find_ngrams(x.split(\" \"), 1))\n",
        "train['sentB_unigram'] = train['sent_B'].map(lambda x: find_ngrams(x.split(\" \"), 1))\n",
        "train['sentA_bigram'] = train['sent_A'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
        "train['sentB_bigram'] = train['sent_B'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
        "\n",
        "test['sentA_unigram'] = test['sent_A'].map(lambda x: find_ngrams(x.split(\" \"), 1))\n",
        "test['sentB_unigram'] = test['sent_B'].map(lambda x: find_ngrams(x.split(\" \"), 1))\n",
        "test['sentA_bigram'] = test['sent_A'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n",
        "test['sentB_bigram'] = test['sent_B'].map(lambda x: find_ngrams(x.split(\" \"), 2))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0QTVJ-Shy6r"
      },
      "source": [
        "train['same_unigram'] = train.apply(lambda x: len(set(x['sentA_unigram']) & set(x['sentB_unigram'])), axis=1)\n",
        "train['same_bigram'] = train.apply(lambda x: len(set(x['sentA_bigram']) & set(x['sentB_bigram'])), axis=1)\n",
        "\n",
        "test['same_unigram'] = test.apply(lambda x: len(set(x['sentA_unigram']) & set(x['sentB_unigram'])), axis=1)\n",
        "test['same_bigram'] = test.apply(lambda x: len(set(x['sentA_bigram']) & set(x['sentB_bigram'])), axis=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RWgdxvGIyK_"
      },
      "source": [
        "## POS Tag: same_postag, fullmatch_postag, fullmatch_postag_pct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPhC_8vAI0BS"
      },
      "source": [
        "*   number of similar POS Tag on sent_A and sent_B (same_postag)\n",
        "*   Number and percentage of similar pair of POS Tag-token (fullmatch_postag, fullmatch_postag_pct)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VApEHCfHkRFV"
      },
      "source": [
        "# Load Pretrained model\n",
        "ct = CRFTagger()\n",
        "pretrained_path = \"all_indo_man_tag_corpus_model.crf.tagger\"\n",
        "ct.set_model_file(resource_path + pretrained_path)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqD9mh1alHeo"
      },
      "source": [
        "def postag_func(sentence):\n",
        "  s_split = sentence.split()\n",
        "\n",
        "  postag = ct.tag_sents([s_split])\n",
        "  return postag"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1DPFmw4lw7R"
      },
      "source": [
        "def generate_postag_func(list_sentences):\n",
        "  list_postag = []\n",
        "  for s in list_sentences:\n",
        "    postag = postag_func(s)\n",
        "    postag = postag[0]\n",
        "    list_postag.append(postag)\n",
        "  \n",
        "  return list_postag"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1i7v7aIpPR0"
      },
      "source": [
        "train['sentA_postag'] = generate_postag_func(train['sent_A'])\n",
        "train['sentB_postag'] = generate_postag_func(train['sent_B'])\n",
        "\n",
        "test['sentA_postag'] = generate_postag_func(test['sent_A'])\n",
        "test['sentB_postag'] = generate_postag_func(test['sent_B'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mcgp1QcqAoY"
      },
      "source": [
        "train['sentA_postag_only'] = train['sentA_postag'].map(lambda l: [b for a, b in l])\n",
        "train['sentB_postag_only'] = train['sentB_postag'].map(lambda l: [b for a, b in l])\n",
        "\n",
        "test['sentA_postag_only'] = test['sentA_postag'].map(lambda l: [b for a, b in l])\n",
        "test['sentB_postag_only'] = test['sentB_postag'].map(lambda l: [b for a, b in l])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REx4F5KRs6o9"
      },
      "source": [
        "Calculate the number of the same POS Tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2tElC4tr-K4"
      },
      "source": [
        "train['same_postag'] = [len(list((Counter(a) & Counter(b)).elements())) for a,b in zip(train[\"sentA_postag_only\"], train[\"sentB_postag_only\"])]\n",
        "test['same_postag'] = [len(list((Counter(a) & Counter(b)).elements())) for a,b in zip(test[\"sentA_postag_only\"], test[\"sentB_postag_only\"])]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTKEoe4Ms1yo"
      },
      "source": [
        "Count pair similarity (full match, POS tag and token) between sent A and sent B and its percentage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsUX1PKLtH4z"
      },
      "source": [
        "def count_match_func(premis, hipotesis):\n",
        "  list_match = []\n",
        "  for p, h in zip(premis, hipotesis):\n",
        "    match = 0\n",
        "    for i in h:\n",
        "      if i in p:\n",
        "        match += 1\n",
        "    list_match.append(match)\n",
        "  \n",
        "  return list_match"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfruYg9atKLH"
      },
      "source": [
        "def similar_postag_func(num_similar, tag):\n",
        "  similar_percentage = []\n",
        "  for n, t in zip(num_similar, tag):\n",
        "    similar = n/len(t)\n",
        "    similar_percentage.append(similar)\n",
        "  \n",
        "  return similar_percentage"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Q6VunGtL8P"
      },
      "source": [
        "train['match_postag'] = count_match_func(train['sentA_postag'].values, train['sentB_postag'].values)\n",
        "test['match_postag'] = count_match_func(test['sentA_postag'].values, test['sentB_postag'].values)\n",
        "\n",
        "train['match_postag_pct'] = similar_postag_func(train['match_postag'].values, train['sentB_postag'].values)\n",
        "test['match_postag_pct'] = similar_postag_func(test['match_postag'].values, test['sentB_postag'].values)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwQAAJsj0YFc"
      },
      "source": [
        "## Negation: num of occurrences in A, num of occurrences in B, difference num of occurrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R46z-eRm0Z8g"
      },
      "source": [
        "The number of occurrences of each negation words in A and B, also the difference number of occurrences. The negation words are: \"tidak\", \"belum\", \"tak\", \"bukan\", dan \"jangan\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-tyq36Y2oCn"
      },
      "source": [
        "train['count_tidak_sentA'] = train['sent_A'].str.count('tidak')\n",
        "train['count_belum_sentA'] = train['sent_A'].str.count('belum')\n",
        "train['count_tak_sentA'] = train['sent_A'].str.count('tak')\n",
        "train['count_bukan_sentA'] = train['sent_A'].str.count('bukan')\n",
        "train['count_jangan_sentA'] = train['sent_A'].str.count('jangan')\n",
        "train['count_tidak_sentB'] = train['sent_B'].str.count('tidak')\n",
        "train['count_belum_sentB'] = train['sent_B'].str.count('belum')\n",
        "train['count_tak_sentB'] = train['sent_B'].str.count('tak')\n",
        "train['count_bukan_sentB'] = train['sent_B'].str.count('bukan')\n",
        "train['count_jangan_sentB'] = train['sent_B'].str.count('jangan')\n",
        "\n",
        "test['count_tidak_sentA'] = test['sent_A'].str.count('tidak')\n",
        "test['count_belum_sentA'] = test['sent_A'].str.count('belum')\n",
        "test['count_tak_sentA'] = test['sent_A'].str.count('tak')\n",
        "test['count_bukan_sentA'] = test['sent_A'].str.count('bukan')\n",
        "test['count_jangan_sentA'] = test['sent_A'].str.count('jangan')\n",
        "test['count_tidak_sentB'] = test['sent_B'].str.count('tidak')\n",
        "test['count_belum_sentB'] = test['sent_B'].str.count('belum')\n",
        "test['count_tak_sentB'] = test['sent_B'].str.count('tak')\n",
        "test['count_bukan_sentB'] = test['sent_B'].str.count('bukan')\n",
        "test['count_jangan_sentB'] = test['sent_B'].str.count('jangan')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawAukzX5viq"
      },
      "source": [
        "train['diff_tidak'] = np.abs(train['count_tidak_sentA']-train['count_tidak_sentB'])\n",
        "train['diff_belum'] = np.abs(train['count_belum_sentA']-train['count_belum_sentB'])\n",
        "train['diff_tak'] = np.abs(train['count_tak_sentA']-train['count_tak_sentB'])\n",
        "train['diff_bukan'] = np.abs(train['count_bukan_sentA']-train['count_bukan_sentB'])\n",
        "train['diff_jangan'] = np.abs(train['count_jangan_sentA']-train['count_jangan_sentB'])\n",
        "\n",
        "test['diff_tidak'] = np.abs(test['count_tidak_sentA']-test['count_tidak_sentB'])\n",
        "test['diff_belum'] = np.abs(test['count_belum_sentA']-test['count_belum_sentB'])\n",
        "test['diff_tak'] = np.abs(test['count_tak_sentA']-test['count_tak_sentB'])\n",
        "test['diff_bukan'] = np.abs(test['count_bukan_sentA']-test['count_bukan_sentB'])\n",
        "test['diff_jangan'] = np.abs(test['count_jangan_sentA']-test['count_jangan_sentB'])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tglrQJGB6N70"
      },
      "source": [
        "## Distance Base: Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07d4GAM6SQX"
      },
      "source": [
        "def levenshtein_distance(a, b):\n",
        "    \"\"\"Return the Levenshtein edit distance between two strings *a* and *b*.\"\"\"\n",
        "    if a == b:\n",
        "        return 0\n",
        "    if len(a) < len(b):\n",
        "        a, b = b, a\n",
        "    if not a:\n",
        "        return len(b)\n",
        "    previous_row = range(len(b) + 1)\n",
        "    for i, column1 in enumerate(a):\n",
        "        current_row = [i + 1]\n",
        "        for j, column2 in enumerate(b):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (column1 != column2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1] "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9kJHlWz6V00"
      },
      "source": [
        "train['lev_dist'] = [levenshtein_distance(a,b) for a, b in zip(train['preprocessed_A'], train['preprocessed_B'])]\n",
        "test['lev_dist'] = [levenshtein_distance(a,b) for a, b in zip(test['preprocessed_A'], test['preprocessed_B'])]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT9VZkSD6xcF"
      },
      "source": [
        "## BLEU Score: bleu_1gram, bleu_2gram, bleu_3gram, bleu_4gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_rBZ-qE6ypM"
      },
      "source": [
        "# cumulative BLEU scores\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "train['bleu_1gram'] = train.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(1, 0, 0, 0),smoothing_function=smoothie), axis=1)\n",
        "train['bleu_2gram'] = train.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie), axis=1)\n",
        "train['bleu_3gram'] = train.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.33, 0.33, 0.33, 0),smoothing_function=smoothie), axis=1)\n",
        "train['bleu_4gram'] = train.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie), axis=1)\n",
        "\n",
        "test['bleu_1gram'] = test.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(1, 0, 0, 0),smoothing_function=smoothie), axis=1)\n",
        "test['bleu_2gram'] = test .apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie), axis=1)\n",
        "test['bleu_3gram'] = test.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.33, 0.33, 0.33, 0),smoothing_function=smoothie), axis=1)\n",
        "test['bleu_4gram'] = test.apply(lambda row: sentence_bleu(row['preprocessed_A'],row['preprocessed_B'],weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie), axis=1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aMrhhng7xPP"
      },
      "source": [
        "## Distance Based: Jaccard Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6dJBMBz7y0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24eab00-665b-4ece-d3ab-9d2b1671db7a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFjcJkai8ccg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e53fc3-ea88-4cf2-d2bd-3b01c16d957f"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImYt3wVM8d6o"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def extract_text_similarity_jaccard (text1, text2):\n",
        "    words_text1 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text1)]\n",
        "    words_text2 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text2)]\n",
        "    nr = len(set(words_text1).intersection(set(words_text2)))\n",
        "    dr = len(set(words_text1).union(set(words_text2)))\n",
        "    jaccard_sim = nr/dr\n",
        "    return jaccard_sim"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMcaAMYQ8g5C"
      },
      "source": [
        "train['jaccard_sim'] = [extract_text_similarity_jaccard(a, b) for a, b in zip(train['preprocessed_A'], train['preprocessed_B'])]\n",
        "test['jaccard_sim'] = [extract_text_similarity_jaccard(a, b) for a, b in zip(test['preprocessed_A'], test['preprocessed_B'])]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC9eis3K81y5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "6dd13b43-6491-4951-b835-35ff16991e2f"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_A</th>\n",
              "      <th>sent_B</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed_A</th>\n",
              "      <th>preprocessed_B</th>\n",
              "      <th>wmd</th>\n",
              "      <th>similar_tok</th>\n",
              "      <th>raw_similar_tok</th>\n",
              "      <th>diff_tok</th>\n",
              "      <th>raw_diff_tok</th>\n",
              "      <th>num_sentA</th>\n",
              "      <th>num_sentB</th>\n",
              "      <th>sentA_unigram</th>\n",
              "      <th>sentB_unigram</th>\n",
              "      <th>sentA_bigram</th>\n",
              "      <th>sentB_bigram</th>\n",
              "      <th>same_unigram</th>\n",
              "      <th>same_bigram</th>\n",
              "      <th>sentA_postag</th>\n",
              "      <th>sentB_postag</th>\n",
              "      <th>sentA_postag_only</th>\n",
              "      <th>sentB_postag_only</th>\n",
              "      <th>same_postag</th>\n",
              "      <th>match_postag</th>\n",
              "      <th>match_postag_pct</th>\n",
              "      <th>count_tidak_sentA</th>\n",
              "      <th>count_belum_sentA</th>\n",
              "      <th>count_tak_sentA</th>\n",
              "      <th>count_bukan_sentA</th>\n",
              "      <th>count_jangan_sentA</th>\n",
              "      <th>count_tidak_sentB</th>\n",
              "      <th>count_belum_sentB</th>\n",
              "      <th>count_tak_sentB</th>\n",
              "      <th>count_bukan_sentB</th>\n",
              "      <th>count_jangan_sentB</th>\n",
              "      <th>diff_tidak</th>\n",
              "      <th>diff_belum</th>\n",
              "      <th>diff_tak</th>\n",
              "      <th>diff_bukan</th>\n",
              "      <th>diff_jangan</th>\n",
              "      <th>lev_dist</th>\n",
              "      <th>bleu_1gram</th>\n",
              "      <th>bleu_2gram</th>\n",
              "      <th>bleu_3gram</th>\n",
              "      <th>bleu_4gram</th>\n",
              "      <th>jaccard_sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pada awalnya bangsa Israel hanya terdiri dari ...</td>\n",
              "      <td>Pada awalnya bangsa Yahudi hanya terdiri dari ...</td>\n",
              "      <td>menolak perubahan teks terakhir oleh istimewa ...</td>\n",
              "      <td>0</td>\n",
              "      <td>awal bangsa israel diri satu kelompok keluarga...</td>\n",
              "      <td>awal bangsa yahudi diri satu kelompok keluarga...</td>\n",
              "      <td>0.989940</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>[(Pada,), (awalnya,), (bangsa,), (Israel,), (h...</td>\n",
              "      <td>[(Pada,), (awalnya,), (bangsa,), (Yahudi,), (h...</td>\n",
              "      <td>[(Pada, awalnya), (awalnya, bangsa), (bangsa, ...</td>\n",
              "      <td>[(Pada, awalnya), (awalnya, bangsa), (bangsa, ...</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>[(Pada, IN), (awalnya, NN), (bangsa, NN), (Isr...</td>\n",
              "      <td>[(Pada, IN), (awalnya, NN), (bangsa, NN), (Yah...</td>\n",
              "      <td>[IN, NN, NN, NNP, RB, VB, IN, CD, NN, NN, IN, ...</td>\n",
              "      <td>[IN, NN, NN, NNP, RB, VB, IN, CD, NN, NN, IN, ...</td>\n",
              "      <td>25</td>\n",
              "      <td>24</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.203704</td>\n",
              "      <td>0.313860</td>\n",
              "      <td>0.321506</td>\n",
              "      <td>0.298068</td>\n",
              "      <td>0.882353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Salah satu tekniknya adalah periplus , deskrip...</td>\n",
              "      <td>Bangsa Romawi memberi sumbangan pada pemetaan ...</td>\n",
              "      <td>sejarah geografi</td>\n",
              "      <td>0</td>\n",
              "      <td>salah satu teknik periplus deskripsi labuh dar...</td>\n",
              "      <td>bangsa romawi beri sumbang meta mereka banyak ...</td>\n",
              "      <td>21.542550</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>24</td>\n",
              "      <td>38</td>\n",
              "      <td>36</td>\n",
              "      <td>12</td>\n",
              "      <td>[(Salah,), (satu,), (tekniknya,), (adalah,), (...</td>\n",
              "      <td>[(Bangsa,), (Romawi,), (memberi,), (sumbangan,...</td>\n",
              "      <td>[(Salah, satu), (satu, tekniknya), (tekniknya,...</td>\n",
              "      <td>[(Bangsa, Romawi), (Romawi, memberi), (memberi...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[(Salah, NN), (satu, CD), (tekniknya, RB), (ad...</td>\n",
              "      <td>[(Bangsa, NNP), (Romawi, NNP), (memberi, VB), ...</td>\n",
              "      <td>[NN, CD, RB, VB, NN, Z, NN, IN, NN, CC, NN, NN...</td>\n",
              "      <td>[NNP, NNP, VB, NN, IN, NN, SC, PRP, CD, VB, NN...</td>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>191</td>\n",
              "      <td>0.220779</td>\n",
              "      <td>0.320370</td>\n",
              "      <td>0.323028</td>\n",
              "      <td>0.297627</td>\n",
              "      <td>0.026316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>definisi anthropologi menurut para ahli</td>\n",
              "      <td>0</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar lukis cir...</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar sukusuku ...</td>\n",
              "      <td>9.921097</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>[(Etnolinguistik,), (antropologi,), (adalah,),...</td>\n",
              "      <td>[(Etnolinguistik,), (antropologi,), (adalah,),...</td>\n",
              "      <td>[(Etnolinguistik, antropologi), (antropologi, ...</td>\n",
              "      <td>[(Etnolinguistik, antropologi), (antropologi, ...</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>[(Etnolinguistik, NN), (antropologi, NN), (ada...</td>\n",
              "      <td>[(Etnolinguistik, NN), (antropologi, NN), (ada...</td>\n",
              "      <td>[NN, NN, VB, NN, SC, VB, NN, IN, NN, CC, NN, N...</td>\n",
              "      <td>[NN, NN, VB, NN, SC, VB, NN, NN, SC, VB, IN, N...</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>0.268657</td>\n",
              "      <td>0.350317</td>\n",
              "      <td>0.341297</td>\n",
              "      <td>0.309585</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sent_A  ... jaccard_sim\n",
              "0  Pada awalnya bangsa Israel hanya terdiri dari ...  ...    0.882353\n",
              "1  Salah satu tekniknya adalah periplus , deskrip...  ...    0.026316\n",
              "2  Etnolinguistik antropologi adalah ilmu yang me...  ...    0.642857\n",
              "\n",
              "[3 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTOKNO9uYIFO"
      },
      "source": [
        "# Modelling Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oShv7ZNo9wtB"
      },
      "source": [
        "## Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydw1QqV29x7f"
      },
      "source": [
        "from scipy.stats import zscore\n",
        "cols_norm = ['wmd',\n",
        "        'similar_tok',\n",
        "        'raw_similar_tok',\n",
        "        'diff_tok',\n",
        "        'raw_diff_tok',\n",
        "        'num_sentA',\n",
        "        'num_sentB',\n",
        "        'same_unigram',\n",
        "        'same_bigram',\n",
        "        'same_postag',\n",
        "        'match_postag',\n",
        "        'match_postag_pct',\n",
        "        'lev_dist',\n",
        "        'bleu_1gram',\n",
        "        'bleu_2gram',\n",
        "        'bleu_3gram',\n",
        "        'bleu_4gram',\n",
        "        'jaccard_sim']\n",
        "\n",
        "norm_train = train.copy()\n",
        "norm_train[cols_norm] = norm_train[cols_norm].apply(zscore)\n",
        "\n",
        "norm_test = test.copy()\n",
        "norm_test[cols_norm] = norm_test[cols_norm].apply(zscore)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znFEJ0pHAYGM"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8vTekdVApax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "12b408d2-def7-4854-b943-dba1d7734ba6"
      },
      "source": [
        "norm_train.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_A</th>\n",
              "      <th>sent_B</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed_A</th>\n",
              "      <th>preprocessed_B</th>\n",
              "      <th>wmd</th>\n",
              "      <th>similar_tok</th>\n",
              "      <th>raw_similar_tok</th>\n",
              "      <th>diff_tok</th>\n",
              "      <th>raw_diff_tok</th>\n",
              "      <th>num_sentA</th>\n",
              "      <th>num_sentB</th>\n",
              "      <th>sentA_unigram</th>\n",
              "      <th>sentB_unigram</th>\n",
              "      <th>sentA_bigram</th>\n",
              "      <th>sentB_bigram</th>\n",
              "      <th>same_unigram</th>\n",
              "      <th>same_bigram</th>\n",
              "      <th>sentA_postag</th>\n",
              "      <th>sentB_postag</th>\n",
              "      <th>sentA_postag_only</th>\n",
              "      <th>sentB_postag_only</th>\n",
              "      <th>same_postag</th>\n",
              "      <th>match_postag</th>\n",
              "      <th>match_postag_pct</th>\n",
              "      <th>count_tidak_sentA</th>\n",
              "      <th>count_belum_sentA</th>\n",
              "      <th>count_tak_sentA</th>\n",
              "      <th>count_bukan_sentA</th>\n",
              "      <th>count_jangan_sentA</th>\n",
              "      <th>count_tidak_sentB</th>\n",
              "      <th>count_belum_sentB</th>\n",
              "      <th>count_tak_sentB</th>\n",
              "      <th>count_bukan_sentB</th>\n",
              "      <th>count_jangan_sentB</th>\n",
              "      <th>diff_tidak</th>\n",
              "      <th>diff_belum</th>\n",
              "      <th>diff_tak</th>\n",
              "      <th>diff_bukan</th>\n",
              "      <th>diff_jangan</th>\n",
              "      <th>lev_dist</th>\n",
              "      <th>bleu_1gram</th>\n",
              "      <th>bleu_2gram</th>\n",
              "      <th>bleu_3gram</th>\n",
              "      <th>bleu_4gram</th>\n",
              "      <th>jaccard_sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pada awalnya bangsa Israel hanya terdiri dari ...</td>\n",
              "      <td>Pada awalnya bangsa Yahudi hanya terdiri dari ...</td>\n",
              "      <td>menolak perubahan teks terakhir oleh istimewa ...</td>\n",
              "      <td>0</td>\n",
              "      <td>awal bangsa israel diri satu kelompok keluarga...</td>\n",
              "      <td>awal bangsa yahudi diri satu kelompok keluarga...</td>\n",
              "      <td>-0.847933</td>\n",
              "      <td>0.611912</td>\n",
              "      <td>0.661902</td>\n",
              "      <td>-0.645613</td>\n",
              "      <td>-0.664710</td>\n",
              "      <td>0.310952</td>\n",
              "      <td>0.710658</td>\n",
              "      <td>[(Pada,), (awalnya,), (bangsa,), (Israel,), (h...</td>\n",
              "      <td>[(Pada,), (awalnya,), (bangsa,), (Yahudi,), (h...</td>\n",
              "      <td>[(Pada, awalnya), (awalnya, bangsa), (bangsa, ...</td>\n",
              "      <td>[(Pada, awalnya), (awalnya, bangsa), (bangsa, ...</td>\n",
              "      <td>0.772835</td>\n",
              "      <td>0.746490</td>\n",
              "      <td>[(Pada, IN), (awalnya, NN), (bangsa, NN), (Isr...</td>\n",
              "      <td>[(Pada, IN), (awalnya, NN), (bangsa, NN), (Yah...</td>\n",
              "      <td>[IN, NN, NN, NNP, RB, VB, IN, CD, NN, NN, IN, ...</td>\n",
              "      <td>[IN, NN, NN, NNP, RB, VB, IN, CD, NN, NN, IN, ...</td>\n",
              "      <td>0.676499</td>\n",
              "      <td>0.728672</td>\n",
              "      <td>0.681871</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.702715</td>\n",
              "      <td>-0.451398</td>\n",
              "      <td>-0.327007</td>\n",
              "      <td>-0.234414</td>\n",
              "      <td>-0.157265</td>\n",
              "      <td>0.772372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Salah satu tekniknya adalah periplus , deskrip...</td>\n",
              "      <td>Bangsa Romawi memberi sumbangan pada pemetaan ...</td>\n",
              "      <td>sejarah geografi</td>\n",
              "      <td>0</td>\n",
              "      <td>salah satu teknik periplus deskripsi labuh dar...</td>\n",
              "      <td>bangsa romawi beri sumbang meta mereka banyak ...</td>\n",
              "      <td>1.814214</td>\n",
              "      <td>-2.060941</td>\n",
              "      <td>-1.733538</td>\n",
              "      <td>5.722080</td>\n",
              "      <td>5.740784</td>\n",
              "      <td>2.858967</td>\n",
              "      <td>-0.210281</td>\n",
              "      <td>[(Salah,), (satu,), (tekniknya,), (adalah,), (...</td>\n",
              "      <td>[(Bangsa,), (Romawi,), (memberi,), (sumbangan,...</td>\n",
              "      <td>[(Salah, satu), (satu, tekniknya), (tekniknya,...</td>\n",
              "      <td>[(Bangsa, Romawi), (Romawi, memberi), (memberi...</td>\n",
              "      <td>-1.138638</td>\n",
              "      <td>-1.173056</td>\n",
              "      <td>[(Salah, NN), (satu, CD), (tekniknya, RB), (ad...</td>\n",
              "      <td>[(Bangsa, NNP), (Romawi, NNP), (memberi, VB), ...</td>\n",
              "      <td>[NN, CD, RB, VB, NN, Z, NN, IN, NN, CC, NN, NN...</td>\n",
              "      <td>[NNP, NNP, VB, NN, IN, NN, SC, PRP, CD, VB, NN...</td>\n",
              "      <td>-0.353180</td>\n",
              "      <td>-1.066087</td>\n",
              "      <td>-1.711456</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.392081</td>\n",
              "      <td>-0.270615</td>\n",
              "      <td>-0.201895</td>\n",
              "      <td>-0.183791</td>\n",
              "      <td>-0.180667</td>\n",
              "      <td>-1.780867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>Etnolinguistik antropologi adalah ilmu yang me...</td>\n",
              "      <td>definisi anthropologi menurut para ahli</td>\n",
              "      <td>0</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar lukis cir...</td>\n",
              "      <td>etnolinguistik antropologi ilmu ajar sukusuku ...</td>\n",
              "      <td>0.308906</td>\n",
              "      <td>0.784355</td>\n",
              "      <td>0.796857</td>\n",
              "      <td>0.946310</td>\n",
              "      <td>0.852380</td>\n",
              "      <td>-0.113717</td>\n",
              "      <td>-0.670750</td>\n",
              "      <td>[(Etnolinguistik,), (antropologi,), (adalah,),...</td>\n",
              "      <td>[(Etnolinguistik,), (antropologi,), (adalah,),...</td>\n",
              "      <td>[(Etnolinguistik, antropologi), (antropologi, ...</td>\n",
              "      <td>[(Etnolinguistik, antropologi), (antropologi, ...</td>\n",
              "      <td>-0.014242</td>\n",
              "      <td>0.015234</td>\n",
              "      <td>[(Etnolinguistik, NN), (antropologi, NN), (ada...</td>\n",
              "      <td>[(Etnolinguistik, NN), (antropologi, NN), (ada...</td>\n",
              "      <td>[NN, NN, VB, NN, SC, VB, NN, IN, NN, CC, NN, N...</td>\n",
              "      <td>[NN, NN, VB, NN, SC, VB, NN, NN, SC, VB, IN, N...</td>\n",
              "      <td>-0.353180</td>\n",
              "      <td>-0.078969</td>\n",
              "      <td>0.816706</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.316245</td>\n",
              "      <td>0.236277</td>\n",
              "      <td>0.373584</td>\n",
              "      <td>0.423815</td>\n",
              "      <td>0.453302</td>\n",
              "      <td>0.058046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sekarang , tidak ada yang tahu pasti kapan sej...</td>\n",
              "      <td>Sejarah dimulai pada awal penciptaan manusia .</td>\n",
              "      <td>1 penggantian istilah quot prasejarah quot den...</td>\n",
              "      <td>0</td>\n",
              "      <td>sekarang ada tahu kapan sejarah mulai</td>\n",
              "      <td>sejarah mulai awal cipta manusia</td>\n",
              "      <td>1.283110</td>\n",
              "      <td>-1.078020</td>\n",
              "      <td>-1.613043</td>\n",
              "      <td>-0.380293</td>\n",
              "      <td>0.009552</td>\n",
              "      <td>-1.387724</td>\n",
              "      <td>-1.284709</td>\n",
              "      <td>[(Sekarang,), (,,), (tidak,), (ada,), (yang,),...</td>\n",
              "      <td>[(Sejarah,), (dimulai,), (pada,), (awal,), (pe...</td>\n",
              "      <td>[(Sekarang, ,), (,, tidak), (tidak, ada), (ada...</td>\n",
              "      <td>[(Sejarah, dimulai), (dimulai, pada), (pada, a...</td>\n",
              "      <td>-1.363517</td>\n",
              "      <td>-1.173056</td>\n",
              "      <td>[(Sekarang, NN), (,, Z), (tidak, NEG), (ada, V...</td>\n",
              "      <td>[(Sejarah, NN), (dimulai, VB), (pada, IN), (aw...</td>\n",
              "      <td>[NN, Z, NEG, VB, SC, VB, RB, NN, NN, VB, Z]</td>\n",
              "      <td>[NN, VB, IN, NN, NN, NN, Z]</td>\n",
              "      <td>-1.382858</td>\n",
              "      <td>-1.245562</td>\n",
              "      <td>-1.591068</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.207005</td>\n",
              "      <td>2.023867</td>\n",
              "      <td>1.774324</td>\n",
              "      <td>1.611511</td>\n",
              "      <td>1.480318</td>\n",
              "      <td>-1.196551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Soekarno tidak memilih bahasanya sendiri , Jaw...</td>\n",
              "      <td>Dengan memilih Bahasa Melayu , para pejuang ke...</td>\n",
              "      <td>bahasa indonesia</td>\n",
              "      <td>0</td>\n",
              "      <td>soekarno pilih bahasa sendiri jawa benar bahas...</td>\n",
              "      <td>pilih bahasa melayu juang merdeka satu seperti...</td>\n",
              "      <td>1.567656</td>\n",
              "      <td>-1.543614</td>\n",
              "      <td>-1.409128</td>\n",
              "      <td>0.150348</td>\n",
              "      <td>0.346683</td>\n",
              "      <td>0.452509</td>\n",
              "      <td>0.403678</td>\n",
              "      <td>[(Soekarno,), (tidak,), (memilih,), (bahasanya...</td>\n",
              "      <td>[(Dengan,), (memilih,), (Bahasa,), (Melayu,), ...</td>\n",
              "      <td>[(Soekarno, tidak), (tidak, memilih), (memilih...</td>\n",
              "      <td>[(Dengan, memilih), (memilih, Bahasa), (Bahasa...</td>\n",
              "      <td>-0.576440</td>\n",
              "      <td>-0.990242</td>\n",
              "      <td>[(Soekarno, NNP), (tidak, NEG), (memilih, VB),...</td>\n",
              "      <td>[(Dengan, SC), (memilih, VB), (Bahasa, NNP), (...</td>\n",
              "      <td>[NNP, NEG, VB, NN, NN, Z, NNP, Z, SC, RB, RB, ...</td>\n",
              "      <td>[SC, VB, NNP, NNP, Z, DT, NN, NN, PR, RB, IN, ...</td>\n",
              "      <td>0.058692</td>\n",
              "      <td>-0.617397</td>\n",
              "      <td>-1.387333</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.335204</td>\n",
              "      <td>-0.673852</td>\n",
              "      <td>-0.658476</td>\n",
              "      <td>-0.635582</td>\n",
              "      <td>-0.616625</td>\n",
              "      <td>-1.400491</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sent_A  ... jaccard_sim\n",
              "0  Pada awalnya bangsa Israel hanya terdiri dari ...  ...    0.772372\n",
              "1  Salah satu tekniknya adalah periplus , deskrip...  ...   -1.780867\n",
              "2  Etnolinguistik antropologi adalah ilmu yang me...  ...    0.058046\n",
              "3  Sekarang , tidak ada yang tahu pasti kapan sej...  ...   -1.196551\n",
              "4  Soekarno tidak memilih bahasanya sendiri , Jaw...  ...   -1.400491\n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZGkJMBVg3C1"
      },
      "source": [
        "# Define all column\n",
        "cols = ['wmd',\n",
        "        'similar_tok',\n",
        "        'raw_similar_tok',\n",
        "        'diff_tok',\n",
        "        'raw_diff_tok',\n",
        "        'num_sentA',\n",
        "        'num_sentB',\n",
        "        'same_unigram',\n",
        "        'same_bigram',\n",
        "        'same_postag',\n",
        "        'match_postag',\n",
        "        'match_postag_pct',\n",
        "        'count_tidak_sentA',\n",
        "        'count_belum_sentA',\n",
        "        'count_tak_sentA',\n",
        "        'count_bukan_sentA',\n",
        "        'count_jangan_sentA',\n",
        "        'count_tidak_sentB',\n",
        "        'count_belum_sentB',\n",
        "        'count_tak_sentB',\n",
        "        'count_bukan_sentB',\n",
        "        'count_jangan_sentB',\n",
        "        'diff_tidak',\n",
        "        'diff_belum',\n",
        "        'diff_tak',\n",
        "        'diff_bukan',\n",
        "        'diff_jangan',\n",
        "        'lev_dist',\n",
        "        'bleu_1gram',\n",
        "        'bleu_2gram',\n",
        "        'bleu_3gram',\n",
        "        'bleu_4gram',\n",
        "        'jaccard_sim']\n",
        "\n",
        "cols_model = cols\n",
        "cols_model.append('preprocessed_A')\n",
        "cols_model.append('preprocessed_B')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9CxCDI9AZfD"
      },
      "source": [
        "x_train = norm_train[cols_model].values\n",
        "x_test = norm_test[cols_model].values\n",
        "\n",
        "y_train = norm_train['label'].values\n",
        "y_test = norm_test['label'].values"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1prXvvMCKBY"
      },
      "source": [
        "## Distance Based: Euclidean Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXFeZooWCNbx"
      },
      "source": [
        "# concatenate premis and hipotesis to get new corpus\n",
        "def get_corpus_func(sentences):\n",
        "  n = len(sentences)\n",
        "\n",
        "  corpus = []\n",
        "  corpus_premis = []\n",
        "  corpus_hipotesis = []\n",
        "  corpus_label = []\n",
        "\n",
        "  for row in sentences:\n",
        "    p = row[-2]\n",
        "    h = row[-1]\n",
        "    c = p + \" \" + h\n",
        "\n",
        "    corpus_premis.append(p)\n",
        "    corpus_hipotesis.append(h)\n",
        "    corpus.append(c)\n",
        "  \n",
        "  return corpus_premis, corpus_hipotesis, corpus"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSVhGq3XCW_d"
      },
      "source": [
        "# function to calculate the euclidean distance\n",
        "def get_feature_func(tfidf_premis, tfidf_hipotesis):\n",
        "  # Euclidean distance between TFIDF vectors for sentence1 and sentence2\n",
        "  tfidf_distance = tfidf_premis - tfidf_hipotesis\n",
        "  tfidf_feature = [np.linalg.norm(tfidf_distance[ind].toarray()) for ind in range(tfidf_distance.shape[0])]\n",
        "  tfidf_feature_array = np.asarray(tfidf_feature).reshape(-1, 1)\n",
        "\n",
        "  return tfidf_feature_array"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gku27ilxCamG"
      },
      "source": [
        "# get corpus for train and test data\n",
        "train_A, train_B, train_corpus = get_corpus_func(x_train)\n",
        "test_A, test_B, test_corpus = get_corpus_func(x_test)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fncJngx2NMIT"
      },
      "source": [
        "# extract TF-IDF feature using default parameter\n",
        "# fit TF-IDF by combine corpus (premis + hipotesis) of train data\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(train_corpus)\n",
        "\n",
        "train_tfidf_A = vectorizer.transform(train_A)\n",
        "train_tfidf_B = vectorizer.transform(train_B)\n",
        "\n",
        "test_tfidf_A = vectorizer.transform(test_A)\n",
        "test_tfidf_B = vectorizer.transform(test_B)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyC7hB6_Nh9k"
      },
      "source": [
        "# get calculated feature\n",
        "train_feature = get_feature_func(train_tfidf_A, train_tfidf_B)\n",
        "test_feature = get_feature_func(test_tfidf_A, test_tfidf_B)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVB9xaAhOEv-"
      },
      "source": [
        "## Token Based: L2 Norm for Vectorize Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27tALdoPFYA"
      },
      "source": [
        "import collections\n",
        "import nltk\n",
        "import os\n",
        "from sklearn import feature_extraction\n",
        "\n",
        "def extract_features(corpus, hipotesis_corpus):\n",
        "    '''Extract TF-IDF features from corpus'''\n",
        "   \n",
        "    # vectorize means we turn non-numerical data into an array of numbers\n",
        "    count_vectorizer = feature_extraction.text.CountVectorizer(ngram_range=(2, 2))\n",
        "    processed_corpus = count_vectorizer.fit_transform(hipotesis_corpus)\n",
        "    processed_corpus = feature_extraction.text.TfidfTransformer().fit_transform(\n",
        "        processed_corpus)\n",
        "\n",
        "    return processed_corpus"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEMh0A7yPIW1"
      },
      "source": [
        "ngram = extract_features(train_corpus, train_B)\n",
        "bigram_train = [np.linalg.norm(ngram[ind].toarray()) for ind in range(ngram.shape[0])]\n",
        "bigram_array_train = np.asarray(bigram_train).reshape(-1, 1)\n",
        "\n",
        "ngram_test = extract_features(train_corpus, test_B)\n",
        "bigram_test = [np.linalg.norm(ngram_test[ind].toarray()) for ind in range(ngram_test.shape[0])]\n",
        "bigram_array_test = np.asarray(bigram_test).reshape(-1, 1)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywvorlVWQeC6"
      },
      "source": [
        "## Concat Features\n",
        "Define features for each group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swD9URFf3vNJ"
      },
      "source": [
        "cols_dist = ['wmd', 'lev_dist', 'jaccard_sim']\n",
        "cols_tok = ['similar_tok', 'raw_similar_tok', 'diff_tok', 'raw_diff_tok', 'num_sentA', 'num_sentB', 'same_unigram', 'same_bigram']\n",
        "cols_pos = ['same_postag', 'match_postag', 'match_postag_pct']\n",
        "cols_neg = ['count_tidak_sentA', 'count_belum_sentA', 'count_tak_sentA', 'count_bukan_sentA', 'count_jangan_sentA', \n",
        "            'count_tidak_sentB', 'count_belum_sentB', 'count_tak_sentB', 'count_bukan_sentB', 'count_jangan_sentB',\n",
        "            'diff_tidak', 'diff_belum', 'diff_tak', 'diff_bukan', 'diff_jangan']\n",
        "cols_bleu = ['bleu_1gram', 'bleu_2gram', 'bleu_3gram', 'bleu_4gram'] "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJypmQW3V1Zi"
      },
      "source": [
        "x_train_dist = train[cols_dist].values\n",
        "x_test_dist = test[cols_dist].values\n",
        "\n",
        "x_train_dist = np.append(x_train_dist, train_feature, axis=1)\n",
        "x_test_dist = np.append(x_test_dist, test_feature, axis=1)\n",
        "\n",
        "x_train_tok = train[cols_tok].values\n",
        "x_test_tok = test[cols_tok].values\n",
        "\n",
        "x_train_tok = np.append(x_train_tok, bigram_array_train, axis=1)\n",
        "x_test_tok = np.append(x_test_tok, bigram_array_test, axis=1)\n",
        "\n",
        "x_train_pos = train[cols_pos].values\n",
        "x_test_pos = test[cols_pos].values\n",
        "\n",
        "x_train_neg = train[cols_neg].values\n",
        "x_test_neg = test[cols_neg].values\n",
        "\n",
        "x_train_bl = train[cols_bleu].values\n",
        "x_test_bl = test[cols_bleu].values"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq2pp27haIos"
      },
      "source": [
        "# Training & Testing Scenario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ppvljdbzfC"
      },
      "source": [
        "import time\n",
        "import sys"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfpdRWkzVv2r"
      },
      "source": [
        "## All Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbnCu9awui0U",
        "outputId": "c751d527-62c0-4941-dcd3-0fcdfa9e0a49"
      },
      "source": [
        "x_train_1 = x_train_dist\n",
        "x_test_1 = x_test_dist\n",
        "\n",
        "x_train_1 = np.append(x_train_1, x_train_tok, axis=1)\n",
        "x_test_1 = np.append(x_test_1, x_test_tok, axis=1)\n",
        "\n",
        "x_train_1 = np.append(x_train_1, x_train_pos, axis=1)\n",
        "x_test_1 = np.append(x_test_1, x_test_pos, axis=1)\n",
        "\n",
        "x_train_1 = np.append(x_train_1, x_train_neg, axis=1)\n",
        "x_test_1 = np.append(x_test_1, x_test_neg, axis=1)\n",
        "\n",
        "x_train_1 = np.append(x_train_1, x_train_bl, axis=1)\n",
        "x_test_1 = np.append(x_test_1, x_test_bl, axis=1)\n",
        "\n",
        "len(x_train_1[0])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIeWVTbmaTut"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5BB7Gk6aYW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db78f427-b142-42fc-cd3f-e7a66a902738"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model1 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_1, y_train)\n",
        "stop = time.time()\n",
        "start1 = time.time()\n",
        "y_pred_logreg1 = logreg_model1.predict(x_test_1)\n",
        "stop1 = time.time()\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(f\"Preidction Time: {stop1 - start1}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model1))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.24411606788635254\n",
            "Preidction Time: 0.0002703666687011719\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwV9pygVaYW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f52a43c-558d-4eec-f743-a866a842c388"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model1 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_1, y_train)\n",
        "stop = time.time()\n",
        "start1 = time.time()\n",
        "y_pred_svm1 = svm_model1.predict(x_test_1)\n",
        "stop1 = time.time()\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(f\"Prediction Time: {stop1 - start1}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model1))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.008749008178710938\n",
            "Prediction Time: 0.0012502670288085938\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uSevp20w_hp",
        "outputId": "ab77a5ca-b058-444c-ae79-f660b291fe7e"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model1 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_1, y_train)\n",
        "stop = time.time()\n",
        "start1 = time.time()\n",
        "y_pred_mlp1 = mlp_model1.predict(x_test_1)\n",
        "stop1 = time.time()\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(f\"Prediction Time: {stop1 - start1}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model1))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 1.2142772674560547\n",
            "Prediction Time: 0.0006837844848632812\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlY6oImRchen"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWuxUwG-as6m",
        "outputId": "0c977e54-99ca-4878-d004-fc07e2f52cf3"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg1, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8276    0.6154    0.7059        39\n",
            "           1     0.7887    0.9180    0.8485        61\n",
            "\n",
            "    accuracy                         0.8000       100\n",
            "   macro avg     0.8082    0.7667    0.7772       100\n",
            "weighted avg     0.8039    0.8000    0.7929       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrY5zf70as69",
        "outputId": "b1023f32-2c90-4bbf-c3d5-cb4ce00d5aff"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm1, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8889    0.6154    0.7273        39\n",
            "           1     0.7945    0.9508    0.8657        61\n",
            "\n",
            "    accuracy                         0.8200       100\n",
            "   macro avg     0.8417    0.7831    0.7965       100\n",
            "weighted avg     0.8313    0.8200    0.8117       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUhmJxwIyZL4",
        "outputId": "189b405f-faf8-4207-b2ae-a36493d2e247"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp1, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7879    0.6667    0.7222        39\n",
            "           1     0.8060    0.8852    0.8438        61\n",
            "\n",
            "    accuracy                         0.8000       100\n",
            "   macro avg     0.7969    0.7760    0.7830       100\n",
            "weighted avg     0.7989    0.8000    0.7964       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUNjsJUjclZl"
      },
      "source": [
        "## Remove Distance Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HVkoN7IbTNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67093e80-9b29-45ec-df46-e256573b2dd8"
      },
      "source": [
        "x_train_2 = x_train_tok\n",
        "x_valid_2 = x_valid_tok\n",
        "x_test_2 = x_test_tok\n",
        "\n",
        "x_train_2 = np.append(x_train_2, x_train_pos, axis=1)\n",
        "x_test_2 = np.append(x_test_2, x_test_pos, axis=1)\n",
        "\n",
        "x_train_2 = np.append(x_train_2, x_train_neg, axis=1)\n",
        "x_test_2 = np.append(x_test_2, x_test_neg, axis=1)\n",
        "\n",
        "x_train_2 = np.append(x_train_2, x_train_bl, axis=1)\n",
        "x_test_2 = np.append(x_test_2, x_test_bl, axis=1)\n",
        "\n",
        "len(x_train_2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOCluNwJco0L"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sd4ItZR0Y2n",
        "outputId": "87fc3a5d-95be-4979-ce05-34ba76c3bda2"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model2 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_2, y_train)\n",
        "stop = time.time()\n",
        "y_pred_logreg2 = logreg_model2.predict(x_test_2)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model2))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.12523984909057617\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCG8z4up0Y2o",
        "outputId": "d985283c-f478-4586-d1c9-2a0465bf1ebc"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model2 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_2, y_train)\n",
        "stop = time.time()\n",
        "y_pred_svm2 = svm_model2.predict(x_test_2)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model2))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.009885549545288086\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dDKhPVl0Y2o",
        "outputId": "1bbf89f2-4a4f-4246-fdd5-5c7fb32689d1"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model2 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_2, y_train)\n",
        "stop = time.time()\n",
        "y_pred_mlp2 = mlp_model2.predict(x_test_2)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model2))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 1.0468215942382812\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nVKqLayc5bf"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4h8SMUP0Y2p",
        "outputId": "93b31158-024a-486e-8d0b-d7b55bfd0075"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg2, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.6154    0.7164        39\n",
            "           1     0.7917    0.9344    0.8571        61\n",
            "\n",
            "    accuracy                         0.8100       100\n",
            "   macro avg     0.8244    0.7749    0.7868       100\n",
            "weighted avg     0.8172    0.8100    0.8023       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn2QDDCz0Y2q",
        "outputId": "67474a24-f552-4436-f3bc-67eb21589aa7"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm2, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8889    0.6154    0.7273        39\n",
            "           1     0.7945    0.9508    0.8657        61\n",
            "\n",
            "    accuracy                         0.8200       100\n",
            "   macro avg     0.8417    0.7831    0.7965       100\n",
            "weighted avg     0.8313    0.8200    0.8117       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jBeYhBJ0Y2q",
        "outputId": "6723b73a-7a99-4466-fac6-7af3005f9d63"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp2, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8214    0.5897    0.6866        39\n",
            "           1     0.7778    0.9180    0.8421        61\n",
            "\n",
            "    accuracy                         0.7900       100\n",
            "   macro avg     0.7996    0.7539    0.7643       100\n",
            "weighted avg     0.7948    0.7900    0.7814       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCdfagkMc9dp"
      },
      "source": [
        "## Remove Token Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt8se6mK92et",
        "outputId": "4eda14a9-0503-439c-a11e-dcbdf36f5aab"
      },
      "source": [
        "x_train_3 = x_train_dist\n",
        "x_test_3 = x_test_dist\n",
        "\n",
        "x_train_3 = np.append(x_train_3, x_train_pos, axis=1)\n",
        "x_test_3 = np.append(x_test_3, x_test_pos, axis=1)\n",
        "\n",
        "x_train_3 = np.append(x_train_3, x_train_neg, axis=1)\n",
        "x_test_3 = np.append(x_test_3, x_test_neg, axis=1)\n",
        "\n",
        "x_train_3 = np.append(x_train_3, x_train_bl, axis=1)\n",
        "x_test_3 = np.append(x_test_3, x_test_bl, axis=1)\n",
        "\n",
        "len(x_train_3[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZRFPNY5dCf5"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAToMvV992ev",
        "outputId": "76ae9c92-6053-4ed6-c728-fe4f8d79299e"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model3 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_3, y_train)\n",
        "stop = time.time()\n",
        "y_pred_logreg3 = logreg_model3.predict(x_test_3)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model3))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.0652322769165039\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmwBhesh92ev",
        "outputId": "ff97e1a7-e932-4841-b6ad-142f43bf5cb5"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model3 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_3, y_train)\n",
        "stop = time.time()\n",
        "y_pred_svm3 = svm_model3.predict(x_test_3)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model3))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.008715629577636719\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT4vRq6B92ev",
        "outputId": "a2ce3373-f08f-4d80-a4b0-adb7a4f25deb"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model3 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_3, y_train)\n",
        "stop = time.time()\n",
        "y_pred_mlp3 = mlp_model3.predict(x_test_3)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model3))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.9032843112945557\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0cYmSpUdVZQ"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sPkwDIp92ew",
        "outputId": "47589c79-6973-4042-fb93-1519f2bdb552"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg3, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7667    0.5897    0.6667        39\n",
            "           1     0.7714    0.8852    0.8244        61\n",
            "\n",
            "    accuracy                         0.7700       100\n",
            "   macro avg     0.7690    0.7375    0.7455       100\n",
            "weighted avg     0.7696    0.7700    0.7629       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1nT98kJ92ew",
        "outputId": "c3ecb1d7-0f1f-4df2-df45-356c06285220"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm3, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8214    0.5897    0.6866        39\n",
            "           1     0.7778    0.9180    0.8421        61\n",
            "\n",
            "    accuracy                         0.7900       100\n",
            "   macro avg     0.7996    0.7539    0.7643       100\n",
            "weighted avg     0.7948    0.7900    0.7814       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbFgUGJA92ew",
        "outputId": "dd5247a4-4d2a-46ae-a5e0-a5c3ba3ae130"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp3, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8065    0.6410    0.7143        39\n",
            "           1     0.7971    0.9016    0.8462        61\n",
            "\n",
            "    accuracy                         0.8000       100\n",
            "   macro avg     0.8018    0.7713    0.7802       100\n",
            "weighted avg     0.8007    0.8000    0.7947       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmVpS077dZ0e"
      },
      "source": [
        "## Remove POS Tag "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjBor4B0_Wy0",
        "outputId": "aa3f15ca-a241-4507-e73f-a7d87ed5addc"
      },
      "source": [
        "x_train_4 = x_train_dist\n",
        "x_test_4 = x_test_dist\n",
        "\n",
        "x_train_4 = np.append(x_train_4, x_train_tok, axis=1)\n",
        "x_test_4 = np.append(x_test_4, x_test_tok, axis=1)\n",
        "\n",
        "x_train_4 = np.append(x_train_4, x_train_neg, axis=1)\n",
        "x_test_4 = np.append(x_test_4, x_test_neg, axis=1)\n",
        "\n",
        "x_train_4 = np.append(x_train_4, x_train_bl, axis=1)\n",
        "x_test_4 = np.append(x_test_4, x_test_bl, axis=1)\n",
        "\n",
        "len(x_train_4[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHgoUSAUdo_d"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rORp0uD_Wy3",
        "outputId": "4b24d74e-c330-4ac0-8f52-feff04ea506b"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model4 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_4, y_train)\n",
        "stop = time.time()\n",
        "y_pred_logreg4 = logreg_model4.predict(x_test_4)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model4))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.1703965663909912\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EO_Jsym_Wy4",
        "outputId": "7021ed4a-c6f0-405e-b337-e96a56a19d76"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model4 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_4, y_train)\n",
        "stop = time.time()\n",
        "y_pred_svm4 = svm_model4.predict(x_test_4)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model4))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.009184122085571289\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w_O2Zts_Wy5",
        "outputId": "2e5c80ec-1048-4f00-fe8f-e74536b02c03"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model4 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_4, y_train)\n",
        "stop = time.time()\n",
        "y_pred_mlp4 = mlp_model4.predict(x_test_4)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model4))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 1.2020533084869385\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ_QfNTdw1W"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vNp508j_Wy6",
        "outputId": "1ac65412-359a-49c6-8b58-6a5bdc388950"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg4, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.6154    0.7164        39\n",
            "           1     0.7917    0.9344    0.8571        61\n",
            "\n",
            "    accuracy                         0.8100       100\n",
            "   macro avg     0.8244    0.7749    0.7868       100\n",
            "weighted avg     0.8172    0.8100    0.8023       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdDYnO5e_Wy7",
        "outputId": "54b03eb4-5623-4e58-add6-772f18e65554"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm4, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.6154    0.7164        39\n",
            "           1     0.7917    0.9344    0.8571        61\n",
            "\n",
            "    accuracy                         0.8100       100\n",
            "   macro avg     0.8244    0.7749    0.7868       100\n",
            "weighted avg     0.8172    0.8100    0.8023       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE21EIyx_Wy7",
        "outputId": "c88ea57e-917d-435c-85a6-bb3c69b07ba0"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp4, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7222    0.6667    0.6933        39\n",
            "           1     0.7969    0.8361    0.8160        61\n",
            "\n",
            "    accuracy                         0.7700       100\n",
            "   macro avg     0.7595    0.7514    0.7547       100\n",
            "weighted avg     0.7678    0.7700    0.7682       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPVGztxfd01z"
      },
      "source": [
        "## Remove Negation Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZQhIBnLA4TS",
        "outputId": "d3f82937-b71d-4a7a-d703-0da1ea2974df"
      },
      "source": [
        "x_train_5 = x_train_dist\n",
        "x_valid_5 = x_valid_dist\n",
        "x_test_5 = x_test_dist\n",
        "\n",
        "x_train_5 = np.append(x_train_5, x_train_tok, axis=1)\n",
        "x_test_5 = np.append(x_test_5, x_test_tok, axis=1)\n",
        "\n",
        "x_train_5 = np.append(x_train_5, x_train_pos, axis=1)\n",
        "x_test_5 = np.append(x_test_5, x_test_pos, axis=1)\n",
        "\n",
        "x_train_5 = np.append(x_train_5, x_train_bl, axis=1)\n",
        "x_test_5 = np.append(x_test_5, x_test_bl, axis=1)\n",
        "\n",
        "len(x_train_5[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO3muo42d8Dd"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-sqX4v4A4TT",
        "outputId": "9c1da4c6-50e9-41ef-bdab-01bb7b2d5017"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model5 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_5, y_train)\n",
        "stop = time.time()\n",
        "y_pred_logreg5 = logreg_model5.predict(x_test_5)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model5))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.13351225852966309\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0wPeBIMA4TU",
        "outputId": "085778d0-7183-4eaa-f545-4d7cfc79cd70"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model5 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_5, y_train)\n",
        "stop = time.time()\n",
        "y_pred_svm5 = svm_model5.predict(x_test_5)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model5))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.009552955627441406\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTk1xfV3A4TU",
        "outputId": "f33abf48-6144-47c4-bbab-c23116e5cf0d"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model5 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_5, y_train)\n",
        "stop = time.time()\n",
        "y_pred_mlp5 = mlp_model5.predict(x_test_5)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model5))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.4472980499267578\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJStfwZGeCd1"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44MzJFZUA4TV",
        "outputId": "07b873e7-8202-47e4-df63-64b2b24eb3fb"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg5, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.6154    0.7164        39\n",
            "           1     0.7917    0.9344    0.8571        61\n",
            "\n",
            "    accuracy                         0.8100       100\n",
            "   macro avg     0.8244    0.7749    0.7868       100\n",
            "weighted avg     0.8172    0.8100    0.8023       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nTiLEn1A4TV",
        "outputId": "eb6b7ae9-b848-456a-cba6-6b3c1ac4c9d3"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm5, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8889    0.6154    0.7273        39\n",
            "           1     0.7945    0.9508    0.8657        61\n",
            "\n",
            "    accuracy                         0.8200       100\n",
            "   macro avg     0.8417    0.7831    0.7965       100\n",
            "weighted avg     0.8313    0.8200    0.8117       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89XL15EA4TW",
        "outputId": "0f1395b5-1975-4587-98dd-a6b3e4b46e9d"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp5, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7812    0.6410    0.7042        39\n",
            "           1     0.7941    0.8852    0.8372        61\n",
            "\n",
            "    accuracy                         0.7900       100\n",
            "   macro avg     0.7877    0.7631    0.7707       100\n",
            "weighted avg     0.7891    0.7900    0.7853       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjja__JNeIJe"
      },
      "source": [
        "## Remove BLEU Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5If6Rp91CRE2",
        "outputId": "2d14b8a1-f2ce-443b-f8d0-59c4c6926f7f"
      },
      "source": [
        "x_train_6 = x_train_dist\n",
        "x_valid_6 = x_valid_dist\n",
        "x_test_6 = x_test_dist\n",
        "\n",
        "x_train_6 = np.append(x_train_6, x_train_tok, axis=1)\n",
        "x_valid_6 = np.append(x_valid_6, x_valid_tok, axis=1)\n",
        "x_test_6 = np.append(x_test_6, x_test_tok, axis=1)\n",
        "\n",
        "x_train_6 = np.append(x_train_6, x_train_pos, axis=1)\n",
        "x_valid_6 = np.append(x_valid_6, x_valid_pos, axis=1)\n",
        "x_test_6 = np.append(x_test_6, x_test_pos, axis=1)\n",
        "\n",
        "x_train_6 = np.append(x_train_6, x_train_neg, axis=1)\n",
        "x_valid_6 = np.append(x_valid_6, x_valid_neg, axis=1)\n",
        "x_test_6 = np.append(x_test_6, x_test_neg, axis=1)\n",
        "\n",
        "len(x_train_6[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9P_ol1ceLxT"
      },
      "source": [
        "### Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt9gNHaHCRE4",
        "outputId": "3d6aff6f-bca7-44ae-f0e9-d03528476f07"
      },
      "source": [
        "# Logistic Regression\n",
        "start = time.time()\n",
        "logreg_model6 = LogisticRegression(random_state=9, max_iter=1000, solver='lbfgs', multi_class='auto').fit(x_train_6, y_train)\n",
        "stop = time.time()\n",
        "y_pred_logreg6 = logreg_model6.predict(x_test_6)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(logreg_model6))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.18917489051818848\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooKFSS47CRE5",
        "outputId": "e38cbe3a-2f1d-4652-8d1b-f9438736629c"
      },
      "source": [
        "# SVM\n",
        "start = time.time()\n",
        "svm_model6 = svm.SVC(random_state=11, kernel = 'rbf').fit(x_train_6, y_train)\n",
        "stop = time.time()\n",
        "y_pred_svm6 = svm_model6.predict(x_test_6)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(svm_model6))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.009670257568359375\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atQf2aRJCRE6",
        "outputId": "ece21a50-972c-488e-8a0c-9ce561cb7bfc"
      },
      "source": [
        "# MLP\n",
        "start = time.time()\n",
        "mlp_model6 = MLPClassifier(random_state=13, max_iter=1000).fit(x_train_6, y_train)\n",
        "stop = time.time()\n",
        "y_pred_mlp6 = mlp_model6.predict(x_test_6)\n",
        "\n",
        "print(f\"Training Time: {stop - start}\")\n",
        "print(\"Size: \"+str(sys.getsizeof(mlp_model6))+ \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Time: 0.5541517734527588\n",
            "Size: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMFanITgeW8Q"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1hPBEDgCRE7",
        "outputId": "57a404c6-a1cd-49df-bd4e-d13c52b4e825"
      },
      "source": [
        "# Logistic Regression\n",
        "print(classification_report(y_test, y_pred_logreg6, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8276    0.6154    0.7059        39\n",
            "           1     0.7887    0.9180    0.8485        61\n",
            "\n",
            "    accuracy                         0.8000       100\n",
            "   macro avg     0.8082    0.7667    0.7772       100\n",
            "weighted avg     0.8039    0.8000    0.7929       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmhW1HiUCRE7",
        "outputId": "0207d302-84d0-4842-b55d-532e9332903c"
      },
      "source": [
        "# SVM\n",
        "print(classification_report(y_test, y_pred_svm6, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8889    0.6154    0.7273        39\n",
            "           1     0.7945    0.9508    0.8657        61\n",
            "\n",
            "    accuracy                         0.8200       100\n",
            "   macro avg     0.8417    0.7831    0.7965       100\n",
            "weighted avg     0.8313    0.8200    0.8117       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjyLkrclCRE8",
        "outputId": "697fc48e-122b-4604-ad10-45da54994634"
      },
      "source": [
        "# MLP\n",
        "print(classification_report(y_test, y_pred_mlp6, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7812    0.6410    0.7042        39\n",
            "           1     0.7941    0.8852    0.8372        61\n",
            "\n",
            "    accuracy                         0.7900       100\n",
            "   macro avg     0.7877    0.7631    0.7707       100\n",
            "weighted avg     0.7891    0.7900    0.7853       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}